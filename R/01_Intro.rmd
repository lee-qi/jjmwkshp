---
title: "Jack Mackerel Assessment Workshop"
subtitle: "Day One: Running the Model"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    theme: journal
---

# Main Objectives

- Process the data that go into the assessment model
- Run the current jack mackerel assessment model
- Understand the main components of the model


# Overview of the Jack Mackerel Assessment Process

The assessment is done every year during the Scientific Committee meeting to provide scientific advice for the management of jack mackerel. The current jack mackerel assessment model was adopted in 2010, with several updates since.

- 2 Benchmark workshops ([2016](https://www.sprfmo.int/meetings/scientific-committee/4th-sc-2016/2016-assessment-workshop/), [2018](https://www.sprfmo.int/meetings/scientific-committee/sc-workshops/scw6/))

- 2 Data workshops ([2015](https://www.sprfmo.int/meetings/scientific-committee/3rd-sc-2015/2015-data-workshop/), [2021](https://www.sprfmo.int/meetings/scientific-committee/sc-workshops/scw11/))


# What Happens During an Update

1. Members send data to Secretariat

1. Data get processed to go into the assessment

1. Data get incrementally introduced to the model (exercise called data bridging)

1. Control file gets updated

1. Assessment gets run

1. Check diagnostics

1. Run (limited number of) sensitivities

1. Choose final model

1. Do projections under four different recruitment scenarios using the final model

1. Do retrospective analysis (both analytical and historical) using the final model

1. Output from "most conservative" model gets put into the harvest control rule

1. Harvest control rule determines catch advice that is presented to the Commission

Example of models run during an update (from SC9, also [here](https://github.com/SPRFMO/jjm/tree/master/assessment))


Model      | Description
-----------|--------------
**Models 0.x**| **Data introductions**
0.00     |  Exact 2020 (single stock `h1` and two-stock `h2`) model and data set through 2020 (mod1.0 from SC08)
0.01     |  As 0.00 but with revised catches through 2020 (currently still estimates)
0.02     |  As 0.01 but with updated 2020 fishery age composition data for N_Chile, SC_Chile, and Offshore_Trawl, and updated 2020 fishery length composition data for FarNorth
0.03     |  As 0.02 but with updated 2020 weight at age data for all fisheries and their associated CPUE indices
0.04     |  As 0.03 but replaced offshore CPUE up to 2020
0.05     |  As 0.04 but with 2021 catch projections
0.06     |  As 0.05 but with updated 2021 fishery age composition data for N_Chile, SC_Chile, and Offshore_Trawl, and updated 2020 fishery length composition data for FarNorth
0.07     |  As 0.06 but but with updated 2021 weight at age data for N_Chile, SC_Chile, and FarNorth fleets, and for their associated CPUE indices
0.08     |  As 0.07 but replaced SC_Chile_CPUE index (traditional absolute scaled CPUE by trip)
0.09     |  As 0.08 but replaced Peru_CPUE index
0.10     |  As 0.09 but with updated AcousN 2021 index, with associated age composition and weight at age
-----------|--------------
**Models 1.x**| **Updated Model and Sensitivities**
1.00     | Update model (selectivity changes, recruitment) to 2021; 0.10 data file
1.01     | As 1.00 but use revised data series "antiguo" of age composition and weight at age data for both Chilean fisheries and both Chilean acoustic surveys (assessment/NewAgeData/AgeDataInAssessment.csv)
1.02     | As 1.01 but incorporate revised (validated) age data for surveys and fleets with M and maturity updated (M=0.35) (NOT RUN)
1.03     | As 1.02 but M=0.45 (NOT RUN)
1.04     | As **1.01** but with increased uncertainty (CV=0.4) for final year CPUE indices
1.05     | As **1.04** but replacing 2020/2021 weight at age with 2019 revised "antiguo" data for N_Chile

# Data Processing
- Members send data to the Secretariat
    - Catch data
        - How is this projected for the current year (Secretariat; check Excel sheet in Teams?)

    - Composition data
        - How is this raised (Niels)
        - Right now done by hand via Excel

    - CPUE
        - Each individual fleet does their own standardisation process, details found in the technical annex
        - Is there a unified repository for these?

- These data are stored on the Teams [Data Repo](https://southpacificrfmo.sharepoint.com/:f:/r/sites/SPRFMOSCJackMackerelWorkingGroup/Shared%20Documents/Data%20repository?csf=1&web=1&e=huLIns)


## Components of the JJM Assessment Model
1. Data file
1. Control file
1. Executable

These files are typically stored in the `jjm/assessment` folder on the Github site. We'll go through these in more detail in the next document.

# Running the Assessment
There are two ways you can run the assessment model- using `R` to run the executable, or using the command line.

## Using R
Pros:

- Automatically generates files needed to create model diagnostics
- Creates a `jjm` object in R

## Using the Command Line
Pros:

- Easier to include ADMB flags (useful for debugging)
    - Examples can be found in the handy dandy [Reference card](http://www.admb-project.org/docs/refcards/admb-additional-reference-card.pdf)

Cons:

- Familiarity with ADMB is required
- Tougher to debug when something goes wrong
- 

```
./jjms -ind h1_1.00.ctl
```

# Assessment Model Outputs

Once the model is run, we'll have
- R.rep
- .cor
- .par
- .rep
- .std
- .yld
